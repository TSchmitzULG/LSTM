{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# two features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn parameters split the training set in kfold in order to test different hyperparameters \n",
    "\n",
    "LSTM3 save the graph and model parameter for [optimizing the inference model](https://stackoverflow.com/questions/45382917/how-to-optimize-for-inference-a-simple-saved-tensorflow-1-0-1-graph) and compute the optimized graph.\n",
    "\n",
    "LSTM4 add 1 features to the input in order to take into account the gain parameters of the amps\n",
    "\n",
    "LSTM5 add dropout\n",
    "\n",
    "LSTM6 learn for a duration and not un number of epoch (easier to compare)\n",
    "\n",
    "LSTM7 add a fully connected layer before LSTM to find a good transfert function for Gain parameters.\n",
    "\n",
    "LSTM8 reshaping is done by GPU (faster, and gain in ram) (datashaping 5)\n",
    "\n",
    "LSTM9 go back to input with 2 features made with gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version 1.3.0 of tensorflow\n",
      "shape input train (793800, 100)\n",
      "shape gain train (793800, 1)\n",
      "Data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/totovai/Documents/Doctorat/NeuralNetwork/env/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -0 calculated \n",
      "Epoch 0 MSE 0.08996526480064004 on test set\n",
      "Epoch -1 calculated \n",
      "Epoch 1 MSE 0.08619705686544066 on test set\n",
      "Epoch -2 calculated \n",
      "Epoch 2 MSE 0.08372346553822115 on test set\n",
      "Epoch -3 calculated \n",
      "Epoch 3 MSE 0.0821866693593894 on test set\n",
      "Epoch -4 calculated \n",
      "Epoch 4 MSE 0.08083818730262059 on test set\n",
      "Epoch -5 calculated \n",
      "Epoch 5 MSE 0.07954625324892638 on test set\n",
      "Epoch -6 calculated \n",
      "Epoch 6 MSE 0.07844999422135748 on test set\n",
      "Epoch -7 calculated \n",
      "Epoch 7 MSE 0.07754130213895938 on test set\n",
      "Epoch -8 calculated \n",
      "Epoch 8 MSE 0.07696899214662671 on test set\n",
      "Epoch -9 calculated \n",
      "Epoch 9 MSE 0.07652826770919081 on test set\n",
      "Epoch -10 calculated \n",
      "Epoch 10 MSE 0.07622249483195627 on test set\n",
      "Epoch -11 calculated \n",
      "Epoch 11 MSE 0.07597567407526942 on test set\n",
      "Epoch -12 calculated \n",
      "Epoch 12 MSE 0.075787891142158 on test set\n",
      "Epoch -13 calculated \n",
      "Epoch 13 MSE 0.07559754975303444 on test set\n",
      "Epoch -14 calculated \n",
      "Epoch 14 MSE 0.07534588327788028 on test set\n",
      "Epoch -15 calculated \n",
      "Epoch 15 MSE 0.07505910383904102 on test set\n",
      "Epoch -16 calculated \n",
      "Epoch 16 MSE 0.07474169276703567 on test set\n",
      "Epoch -17 calculated \n",
      "Epoch 17 MSE 0.07449905360975578 on test set\n",
      "Epoch -18 calculated \n",
      "Epoch 18 MSE 0.07422719627275311 on test set\n",
      "Epoch -19 calculated \n",
      "Epoch 19 MSE 0.07393489289680151 on test set\n",
      "Epoch -20 calculated \n",
      "Epoch 20 MSE 0.07371547447987384 on test set\n",
      "Epoch -21 calculated \n",
      "Epoch 21 MSE 0.07354249044375155 on test set\n",
      "Epoch -22 calculated \n",
      "Epoch 22 MSE 0.07335084340768767 on test set\n",
      "Epoch -23 calculated \n",
      "Epoch 23 MSE 0.07312584325421928 on test set\n",
      "Epoch -24 calculated \n",
      "Epoch 24 MSE 0.07263194156486553 on test set\n",
      "Epoch -25 calculated \n",
      "Epoch 25 MSE 0.07259151922876572 on test set\n",
      "Epoch -26 calculated \n",
      "Epoch 26 MSE 0.07263410535200811 on test set\n",
      "Epoch -27 calculated \n",
      "Epoch 27 MSE 0.07243090250398534 on test set\n",
      "Epoch -28 calculated \n",
      "Epoch 28 MSE 0.07195137924249971 on test set\n",
      "Epoch -29 calculated \n",
      "Epoch 29 MSE 0.07182169765465336 on test set\n",
      "Epoch -30 calculated \n",
      "Epoch 30 MSE 0.07161838957356491 on test set\n",
      "Epoch -31 calculated \n",
      "Epoch 31 MSE 0.07360544030793385 on test set\n",
      "Epoch -32 calculated \n",
      "Epoch 32 MSE 0.07376925239103561 on test set\n",
      "Epoch -33 calculated \n",
      "Epoch 33 MSE 0.07298757003149464 on test set\n",
      "Epoch -34 calculated \n",
      "Epoch 34 MSE 0.07283265866916376 on test set\n",
      "Epoch -35 calculated \n",
      "Epoch 35 MSE 0.07235341116766549 on test set\n",
      "Epoch -36 calculated \n",
      "Epoch 36 MSE 0.07142752426868132 on test set\n",
      "Epoch -37 calculated \n",
      "Epoch 37 MSE 0.07127018460625888 on test set\n",
      "Epoch -38 calculated \n",
      "Epoch 38 MSE 0.0710390870458031 on test set\n",
      "Epoch -39 calculated \n",
      "Epoch 39 MSE 0.0712744911677859 on test set\n",
      "Epoch -40 calculated \n",
      "Epoch 40 MSE 0.07071703271122692 on test set\n",
      "Epoch -41 calculated \n",
      "Epoch 41 MSE 0.07090889465748021 on test set\n",
      "Epoch -42 calculated \n",
      "Epoch 42 MSE 0.07101995574165454 on test set\n",
      "Epoch -43 calculated \n",
      "Epoch 43 MSE 0.07059917442593985 on test set\n",
      "Epoch -44 calculated \n",
      "Epoch 44 MSE 0.0712044428177219 on test set\n",
      "Epoch -45 calculated \n",
      "Epoch 45 MSE 0.07081246328280383 on test set\n",
      "Epoch -46 calculated \n",
      "Epoch 46 MSE 0.07261052601571703 on test set\n",
      "Epoch -47 calculated \n",
      "Epoch 47 MSE 0.07140273592318766 on test set\n",
      "Epoch -48 calculated \n",
      "Epoch 48 MSE 0.07449831039524744 on test set\n",
      "Epoch -49 calculated \n",
      "Epoch 49 MSE 0.07375923729978207 on test set\n",
      "Epoch -50 calculated \n",
      "Epoch 50 MSE 0.07020044645287433 on test set\n",
      "Epoch -51 calculated \n",
      "Epoch 51 MSE 0.06959304193325865 on test set\n",
      "Epoch -52 calculated \n",
      "Epoch 52 MSE 0.07348075051120646 on test set\n",
      "Epoch -53 calculated \n",
      "Epoch 53 MSE 0.06937545982007806 on test set\n",
      "Epoch -54 calculated \n",
      "Epoch 54 MSE 0.07147796607888326 on test set\n",
      "Epoch -55 calculated \n",
      "Epoch 55 MSE 0.07432317865984088 on test set\n",
      "Epoch -56 calculated \n",
      "Epoch 56 MSE 0.0683489549911189 on test set\n",
      "Epoch -57 calculated \n",
      "Epoch 57 MSE 0.06783063380578234 on test set\n",
      "Epoch -58 calculated \n",
      "Epoch 58 MSE 0.06918261744259076 on test set\n",
      "Epoch -59 calculated \n",
      "Epoch 59 MSE 0.06783778574463023 on test set\n",
      "Epoch -60 calculated \n",
      "Epoch 60 MSE 0.06956966394073125 on test set\n",
      "Epoch -61 calculated \n",
      "Epoch 61 MSE 0.06849400437391774 on test set\n",
      "Epoch -62 calculated \n",
      "Epoch 62 MSE 0.0674536992812845 on test set\n",
      "Epoch -63 calculated \n",
      "Epoch 63 MSE 0.06777316238388148 on test set\n",
      "Epoch -64 calculated \n",
      "Epoch 64 MSE 0.06847405955091543 on test set\n",
      "Epoch -65 calculated \n",
      "Epoch 65 MSE 0.06763552266100292 on test set\n",
      "Epoch -66 calculated \n",
      "Epoch 66 MSE 0.06764203641994262 on test set\n",
      "Epoch -67 calculated \n",
      "Epoch 67 MSE 0.06742244120376194 on test set\n",
      "Epoch -68 calculated \n",
      "Epoch 68 MSE 0.0680114909666305 on test set\n",
      "Epoch -69 calculated \n",
      "Epoch 69 MSE 0.06787487776531184 on test set\n",
      "Epoch -70 calculated \n",
      "Epoch 70 MSE 0.06692279379563523 on test set\n",
      "Epoch -71 calculated \n",
      "Epoch 71 MSE 0.06716073169268598 on test set\n",
      "Epoch -72 calculated \n",
      "Epoch 72 MSE 0.06731928837385735 on test set\n",
      "Epoch -73 calculated \n",
      "Epoch 73 MSE 0.068229950269506 on test set\n",
      "Epoch -74 calculated \n",
      "Epoch 74 MSE 0.0684517764886999 on test set\n",
      "Epoch -75 calculated \n",
      "Epoch 75 MSE 0.06645816893347074 on test set\n",
      "Epoch -76 calculated \n",
      "Epoch 76 MSE 0.06714848369228088 on test set\n",
      "Epoch -77 calculated \n",
      "Epoch 77 MSE 0.06662911043871227 on test set\n",
      "Epoch -78 calculated \n",
      "Epoch 78 MSE 0.06701842839742972 on test set\n",
      "Epoch -79 calculated \n",
      "Epoch 79 MSE 0.06675210528771848 on test set\n",
      "Epoch -80 calculated \n",
      "Epoch 80 MSE 0.06603179572741241 on test set\n",
      "Epoch -81 calculated \n",
      "Epoch 81 MSE 0.06604887096384565 on test set\n",
      "Epoch -82 calculated \n",
      "Epoch 82 MSE 0.06585173232447007 on test set\n",
      "Epoch -83 calculated \n",
      "Epoch 83 MSE 0.06557916235755316 on test set\n",
      "Epoch -84 calculated \n",
      "Epoch 84 MSE 0.0650534606260979 on test set\n",
      "Epoch -85 calculated \n",
      "Epoch 85 MSE 0.0654527486936196 on test set\n",
      "Epoch -86 calculated \n",
      "Epoch 86 MSE 0.0653619161815469 on test set\n",
      "Epoch -87 calculated \n",
      "Epoch 87 MSE 0.0655816995856488 on test set\n",
      "Epoch -88 calculated \n",
      "Epoch 88 MSE 0.06570294345191763 on test set\n",
      "Epoch -89 calculated \n",
      "Epoch 89 MSE 0.06518909290054535 on test set\n",
      "Epoch -90 calculated \n",
      "Epoch 90 MSE 0.06548506058556855 on test set\n",
      "Epoch -91 calculated \n",
      "Epoch 91 MSE 0.06526519924016946 on test set\n",
      "Epoch -92 calculated \n",
      "Epoch 92 MSE 0.06513859925906897 on test set\n",
      "Epoch -93 calculated \n",
      "Epoch 93 MSE 0.0651075429219993 on test set\n",
      "Epoch -94 calculated \n",
      "Epoch 94 MSE 0.06457711290302177 on test set\n",
      "Epoch -95 calculated \n",
      "Epoch 95 MSE 0.06491763715343112 on test set\n",
      "Epoch -96 calculated \n",
      "Epoch 96 MSE 0.06492210374807557 on test set\n",
      "Epoch -97 calculated \n",
      "Epoch 97 MSE 0.06523730786964882 on test set\n",
      "Epoch -98 calculated \n",
      "Epoch 98 MSE 0.06463410904857571 on test set\n",
      "Epoch -99 calculated \n",
      "Epoch 99 MSE 0.06443559741811357 on test set\n",
      "Epoch -100 calculated \n",
      "Epoch 100 MSE 0.06409528214258033 on test set\n",
      "Epoch -101 calculated \n",
      "Epoch 101 MSE 0.06461238635429939 on test set\n",
      "Epoch -102 calculated \n",
      "Epoch 102 MSE 0.06571125391935906 on test set\n",
      "Epoch -103 calculated \n",
      "Epoch 103 MSE 0.06414326443987109 on test set\n",
      "Epoch -104 calculated \n",
      "Epoch 104 MSE 0.06412020984823182 on test set\n",
      "Epoch -105 calculated \n",
      "Epoch 105 MSE 0.06409903790654221 on test set\n",
      "Epoch -106 calculated \n",
      "Epoch 106 MSE 0.06442247787155372 on test set\n",
      "Training duration 15:03:01 \n",
      "Number of training variable 1096\n",
      "INFO:tensorflow:Restoring parameters from experiments/2017-10-05-12-36/temp/myFinalModel.ckpt\n",
      "INFO:tensorflow:Froze 4 variables.\n",
      "Converted 4 variables to const ops.\n",
      "159 ops in the final graph.\n",
      "done, good job kids\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"lstm for guitar signal with input feature for gain parameters\"\"\"\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('codes')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from DataShaping5 import *\n",
    "from SavePerf import *\n",
    "import scipy.io.wavfile\n",
    "import time\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "modelName = \"LSTM9\"\n",
    "# create directory experiment\n",
    "date = time.strftime(\"%Y-%m-%d-%H-%M\")\n",
    "path = os.path.join(\"experiments\",date)\n",
    "if not os.path.isdir(path):\n",
    "    os.makedirs(path)\n",
    "    pathTemp = os.path.join(path,'temp')\n",
    "    os.makedirs(pathTemp)\n",
    "pathLog = 'tf_logs'\n",
    "if not os.path.isdir(pathLog):\n",
    "    os.makedirs(pathLog)\n",
    "pathLog = \"{}/run-{}/\".format(pathLog,date)\n",
    "\n",
    "version = tf.__version__\n",
    "print (\"version {} of tensorflow\".format(version))\n",
    "#############################\n",
    "# Model parameters\n",
    "#############################\n",
    "trainTestRatio = 0.9\n",
    "maxSize = 44100*11.4*10\n",
    "num_step = 100\n",
    "num_hidden = 15\n",
    "num_class = 1\n",
    "num_feature = 2\n",
    "batch_size = 100\n",
    "num_epoch = 1000\n",
    "trainDuration = 15*60*60\n",
    "trainDropout = 0.0\n",
    "n_layer = 1 # stack severals LSTM cells\n",
    "fileName = 'datasets/trainingEnglGain.mat'\n",
    "fileNameValidation = 'datasets/validationEnglGain.mat'\n",
    "\n",
    "\n",
    "#############################\n",
    "# Charging data\n",
    "#############################\n",
    "matrix = sio.loadmat(fileName)\n",
    "matrix = matrix['mat']\n",
    "if maxSize ==0:\n",
    "    maxSize = len(matrix)\n",
    "    print(maxSize)\n",
    "# to do shuffle matrix by num_step length\n",
    "train_input,train_output,train_gain,test_input, test_output, test_gain = splitShuffleData(matrix,num_step,trainTestRatio,maxSize)\n",
    "print(\"shape input train {}\".format(np.shape(train_input)))\n",
    "print(\"shape gain train {}\".format(np.shape(train_gain)))\n",
    "numTrain = len(train_output)\n",
    "print (\"Data loaded\")\n",
    "#######################\n",
    "#Graph\n",
    "#######################\n",
    "\n",
    "G = tf.Graph()\n",
    "with G.as_default():\n",
    "    with tf.name_scope(\"placeHolder\"):\n",
    "        data = tf.placeholder(tf.float32, [None, num_step], name =\"data\") #Number of examples, number of input step (time step), dimension of each input\n",
    "        target = tf.placeholder(tf.float32, [None, num_class],name = \"target\") # batchSize, nbClass\n",
    "        dropout = tf.placeholder(tf.float32,name=\"dropout\")\n",
    "        gain = tf.placeholder(tf.float32,[None,1],name=\"gain\")\n",
    "        gainShaped = tf.tile(gain, [1, num_step])\n",
    "        dataGain = tf.transpose(tf.stack([data,gainShaped],axis=0), perm=[1, 2, 0])\n",
    "        dataGainShaped = tf.reshape(dataGain,[tf.shape(data)[0],num_step,num_feature])\n",
    "    \n",
    "    def lstm_cell():\n",
    "        cell = tf.contrib.rnn.LSTMCell(num_hidden,state_is_tuple=True,activation = tf.nn.tanh)\n",
    "        return tf.contrib.rnn.DropoutWrapper(cell,input_keep_prob=1-dropout)\n",
    "    \n",
    "    multiLayerCell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(n_layer)] )\n",
    "    #cell = tf.contrib.rnn.GRUCell(num_hidden)\n",
    "    with tf.name_scope(\"extractLastValueLSTM\"):\n",
    "        val, state = tf.nn.dynamic_rnn(multiLayerCell, dataGainShaped, dtype=tf.float32) # val dim is [batchSize,sequencelength, numhidden]\n",
    "    # Let's first fetch the last index of seq length\n",
    "    # last_index would have a scalar value\n",
    "        last_index = tf.shape(val)[1] - 1\n",
    "    # Then let's reshape the output to [sequence_length,batch_size,numhidden]\n",
    "    # for convenience\n",
    "        val = tf.transpose(val,[1,0,2])\n",
    "    # Last state of all batches\n",
    "        last = tf.nn.embedding_lookup(val,last_index) # tensor [batchsize,numhidden]\n",
    "\n",
    "    #Output layer RNN in new feed NN\n",
    "    #weight = tf.Variable(tf.truncated_normal([num_hidden, int(target.get_shape()[1])],mean =0,stddev=np.sqrt(12/(num_hidden+num_class))))\n",
    "    with tf.name_scope(\"FCLSTMtoTarget\"):\n",
    "        weight = tf.get_variable(\"weight\", shape=[num_hidden, int(target.get_shape()[1])], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[target.get_shape()[1]]))\n",
    "\n",
    "    prediction = tf.nn.elu((tf.add(tf.matmul(last, weight) , bias)),name = \"prediction\") #[batchSize,nclass]\n",
    "    #logits = (tf.add(tf.matmul(last, weight) , bias)) #[batchSize,nclass]\n",
    "    #prediction = tf.nn.elu(logits)\n",
    "    #Loss\n",
    "    MSE = tf.reduce_mean(tf.square(prediction-target))\n",
    "    #cross_entropy = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,targets=target))\n",
    "\n",
    "    #cross_entropy = -tf.reduce_sum(target * tf.log(tf.clip_by_value(prediction,1e-10,1.0)))\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    #optimizer = tf.train.RMSPropOptimizer(0.0005)\n",
    "    minimize = optimizer.minimize(MSE)\n",
    "\n",
    "    mse_summary = tf.summary.scalar('MSE',MSE)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver() # save variable, use saver.restore(sess,\"date/tmp/my_model.ckpt\") instead of sess.run(init_op)\n",
    "\n",
    "##############################\n",
    "# Execution du graphe\n",
    "##############################\n",
    "    \n",
    "\n",
    "with tf.Session(graph=G) as sess:\n",
    "    #restorePath = os.path.join('2017-09-11-18-07','temp','my_model.ckpt')\n",
    "    #saver.restore(sess,restorePath)\n",
    "    sess.run(init_op)\n",
    "    train_writer = tf.summary.FileWriter(pathLog+'train',graph =tf.get_default_graph())\n",
    "    test_writer = tf.summary.FileWriter(pathLog+'test')\n",
    "\n",
    "    no_of_batches = int(np.floor((numTrain)/batch_size)) # numtrain -numstep ?\n",
    "    no_of_batchesTest = int(np.floor((len(test_input))/batch_size))\n",
    "    tStart = time.clock()\n",
    "    epoch =0\n",
    "    for epoch in range(num_epoch):\n",
    "        if (time.clock()-tStart < trainDuration):\n",
    "            ptr = 0\n",
    "            if epoch % 10==0 :\n",
    "                tf.train.write_graph(sess.graph_def,\"{}/\".format(pathTemp),'myGraph.pb',as_text=False)\n",
    "                save_path = saver.save(sess,os.path.join(pathTemp,'my_model.ckpt'))\n",
    "            for j in range(no_of_batches):\n",
    "                inp, out,gainp = train_input[ptr:ptr+batch_size],train_output[ptr:ptr+batch_size],train_gain[ptr:ptr+batch_size]\n",
    "                ptr+=batch_size\n",
    "                if j % np.floor(trainTestRatio*10) ==0 :\n",
    "                    _,summary_str = sess.run([minimize,summary_op],{data: inp, target: out , dropout : trainDropout, gain:gainp})\n",
    "                    step = epoch*no_of_batches+j\n",
    "                    train_writer.add_summary(summary_str,step)\n",
    "                else :\n",
    "                    sess.run([minimize],{data: inp, target: out , dropout : trainDropout, gain:gainp})\n",
    "                   \n",
    "                #print value\n",
    "            print (\"Epoch -{} calculated \".format(epoch))\n",
    "            pMSE = 0\n",
    "            ptr2 = 0\n",
    "            for k in range(no_of_batchesTest):\n",
    "                pMSETemp,summary_str = sess.run([MSE,summary_op],{data: test_input[ptr2:ptr2+batch_size] , target: test_output[ptr2:ptr2+batch_size] , dropout : 0.0, gain:test_gain[ptr2:ptr2+batch_size] })\n",
    "                pMSE += pMSETemp\n",
    "                ptr2+=batch_size\n",
    "                step = epoch*no_of_batchesTest+k\n",
    "                test_writer.add_summary(summary_str,step*10*trainTestRatio)\n",
    "            print(\"Epoch {} MSE {} on test set\".format(epoch,np.sqrt(pMSE/no_of_batchesTest)))\n",
    "        else : break # break the while loop if number of epoch is reached\n",
    "    tStop = time.clock()\n",
    "    trainTime = time.strftime(\"%H:%M:%S \", time.gmtime(tStop-tStart))\n",
    "    \n",
    "    #######################\n",
    "    # Save GraphDef\n",
    "    #######################\n",
    "    tf.train.write_graph(sess.graph_def,\"{}/\".format(pathTemp),'myFinalGraph.pb',as_text=False)\n",
    "    # Save checkpoint\n",
    "    save_path = saver.save(sess,os.path.join(pathTemp,'myFinalModel.ckpt'))\n",
    "    print (\"Training duration {}\".format(trainTime))\n",
    "    totalParameters =np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.trainable_variables()])\n",
    "    print(\"Number of training variable {}\".format(totalParameters))\n",
    "    # log\n",
    "    infoLog={}\n",
    "    infoLog[\"path\"] = path\n",
    "    infoLog[\"MSE\"] = np.sqrt(pMSE/no_of_batchesTest)\n",
    "    infoLog[\"num_step\"] = num_step\n",
    "    infoLog[\"num_hidden\"] = num_hidden\n",
    "    infoLog[\"num_epoch\"] = epoch\n",
    "    infoLog[\"batch_size\"] = batch_size\n",
    "    infoLog[\"maxSize\"] = maxSize\n",
    "    infoLog[\"duration\"] = trainTime\n",
    "    infoLog[\"totalParameters\"] = totalParameters\n",
    "    infoLog[\"version\"] = version\n",
    "    infoLog[\"n_layer\"] = n_layer\n",
    "    infoLog[\"trainDropout\"] = trainDropout\n",
    "    infoLog[\"nameModel\"] = modelName\n",
    "    logPerf2(infoLog)\n",
    "    # freeze graph\n",
    "    input_graph_path = \"{}/\".format(pathTemp)+'myFinalGraph.pb'\n",
    "    checkpoint_path = \"{}/\".format(pathTemp)+'myFinalModel.ckpt'\n",
    "    input_saver_def_path = \"\"\n",
    "    input_binary = True\n",
    "    output_node_names = \"prediction\"\n",
    "    restore_op_name = \"save/restore_all\"\n",
    "    filename_tensor_name = \"save/Const:0\"\n",
    "    output_frozen_graph_name = \"{}/\".format(pathTemp)+'frozenModel.pb'\n",
    "    # output_optimized_graph_name = 'optimized_'+MODEL_NAME+'.pb'\n",
    "    clear_devices = True\n",
    "    freeze_graph.freeze_graph(input_graph_path, input_saver_def_path,\n",
    "                          input_binary, checkpoint_path, output_node_names,\n",
    "                          restore_op_name, filename_tensor_name,\n",
    "                          output_frozen_graph_name, clear_devices, \"\")\n",
    "   \n",
    "    \n",
    "    ###############################\n",
    "    #   Testing\n",
    "    ###############################\n",
    "    matrixVal = sio.loadmat(fileNameValidation)\n",
    "    matrixVal = matrixVal['validation']  \n",
    "    # shape validation test\n",
    "    val_input,val_output,val_gain = shapeData(matrixVal,num_step,maxSize)\n",
    "    lPrediction = []\n",
    "    lTarget = []\n",
    "    ptr3 = 0\n",
    "    no_of_batchesVal = int(np.floor((len(val_input))/batch_size))\n",
    "    for k in range(no_of_batchesVal):\n",
    "        pPrediction,pTarget = sess.run([prediction,target],{data: val_input[ptr3:ptr3+batch_size], target: val_output[ptr3:ptr3+batch_size], dropout : 0.0, gain : val_gain[ptr3:ptr3+batch_size]}) \n",
    "        lPrediction.append(pPrediction)\n",
    "        lTarget.append(pTarget)   \n",
    "        ptr3+=batch_size\n",
    "    #plt.show()scree\n",
    "    predictionArray = np.array(lPrediction,dtype=np.float32).ravel()\n",
    "    targetArray = np.array(lTarget,dtype=np.float32).ravel()\n",
    "    scipy.io.wavfile.write(os.path.join(path,'prediction.wav'),44100,predictionArray)\n",
    "    scipy.io.wavfile.write(os.path.join(path,'target.wav'),44100,targetArray)\n",
    "\n",
    "    numSampleShow = 44100\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot(predictionArray[:numSampleShow],label='prediction')\n",
    "    ax.plot(targetArray[:numSampleShow],label='target')\n",
    "    ax.legend()\n",
    "    nameFigEstimation = os.path.join(path,\"targetVsPrediction.pickle\")\n",
    "    #fig.savefig('estimation'+time.strftime(\"%Y-%m-%d-%H-%M\")+'.png')\n",
    "    pickle.dump(ax,open(nameFigEstimation, 'wb'))\n",
    "    \n",
    "print (\"done, good job kids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
